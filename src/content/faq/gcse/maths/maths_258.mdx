---
title: "How do you find the coordinates of a point given its distance from the origin?"
summary: "To determine a point's coordinates based on its distance from the origin, apply the Pythagorean theorem or trigonometric methods."
author: "Prof. Alan Smith"
degree: "PhD in Physics, University of Cambridge"
tutor_type: "A-Level Physics Tutor"
date: 2024-08-28
---

To determine the coordinates of a point based on its distance from the origin, you can utilize either the Pythagorean theorem or trigonometric principles.

When you know the distance from the origin, you can visualize this distance as the hypotenuse of a right-angled triangle. The origin is located at the point $(0,0)$, and the point in question lies on a circle centered at the origin, with a radius equal to the specified distance. If you also know the angle that the line connecting the origin to the point makes with the positive x-axis, you can apply trigonometry to find the precise coordinates of the point.

For instance, if the distance (also referred to as the radius) is given as $r$ and the angle is denoted by $\theta$, the coordinates $(x, y)$ can be calculated using the following formulas:

$$
x = r \cos(\theta)
$$

$$
y = r \sin(\theta)
$$

If the angle is unknown but you have either the x or y coordinate, you can still employ the Pythagorean theorem to find the missing coordinate. For example, if you know the x-coordinate $x$ and the distance $r$, you can determine the y-coordinate using:

$$
y = \sqrt{r^2 - x^2}
$$

Conversely, if you have the y-coordinate $y$ and the distance $r$, you can find the x-coordinate with:

$$
x = \sqrt{r^2 - y^2}
$$

It is important to note that there may be two potential points (one in each of the relevant quadrants) that satisfy these conditions. Therefore, be sure to consider both the positive and negative values of the square root when determining the possible coordinates. This approach will enable you to accurately locate the point on the coordinate plane.
    