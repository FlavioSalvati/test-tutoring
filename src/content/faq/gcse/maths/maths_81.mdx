---
title: "How do you determine the maximum possible error in a measurement?"
summary: "The maximum error in a measurement equals half of the smallest measurement unit employed, indicating the precision limit of the measurement process."
author: "Dr. Angela Davis"
degree: "MSc in Mathematics, University of Bristol"
tutor_type: "GCSE Maths Tutor"
date: 2024-04-28
---

The maximum possible error in a measurement is defined as half of the smallest unit of measurement utilized.

When you take a measurement, the precision of your measuring tool determines the smallest increment you can accurately measure. For instance, if you are using a ruler that is marked in millimeters, the smallest unit is $1 \, \text{mm}$. Consequently, the maximum possible error is half of this smallest unit, as the true value could vary by up to half a unit from the measured value. Therefore, if your ruler measures to the nearest millimeter, the maximum possible error is $0.5 \, \text{mm}$.

To illustrate this concept, consider a scenario where you measure a length and obtain a result of $10 \, \text{mm}$. The actual length could range from $9.5 \, \text{mm}$ to $10.5 \, \text{mm}$. This range arises from the limitations of your measuring tool. Thus, the maximum possible error is expressed as ±$0.5 \, \text{mm}$.

This principle is applicable to any measuring instrument. For example, if you are using a digital scale that measures to the nearest gram, the maximum possible error would be ±$0.5 \, \text{g}$. Similarly, if a thermometer measures to the nearest degree, the maximum possible error is ±$0.5 \, \text{°C}$. It is important to remember that the maximum possible error is always half of the smallest unit of measurement, ensuring that you account for the precision limits of your measuring instrument.
    