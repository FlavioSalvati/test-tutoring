---
title: "What structures underpin neural networks in AI?"
summary: "Neural networks in AI are underpinned by nodes or neurons, weights, biases, and activation functions."
author: "Prof. Ava Johnson"
degree: "PhD in Cybersecurity, University of Manchester"
tutor_type: "IB Computer Science Tutor"
date: 2024-02-21
---

Neural networks, also known as artificial neural networks (ANNs), are a fundamental aspect of artificial intelligence (AI). They are designed to emulate the human brain's capacity for learning and decision-making. The core components that constitute these networks include nodes (or neurons), weights, biases, and activation functions.

**Nodes (or Neurons)**  
Nodes, often referred to as neurons, are the basic building blocks of a neural network. Inspired by biological neurons in the human brain, these units are responsible for processing input data. Each node receives input from other nodes or from external sources and computes an output. Each input is associated with a weight, denoted as $w$, which reflects its significance relative to other inputs. The node processes the weighted sum of its inputs through a function—typically a non-linear activation function—and generates an output.

**Weights**  
Weights are vital components of neural networks, as they modulate the influence of inputs on a neuron's output. Throughout the training phase, these weights are optimized to minimize the discrepancy between the actual output and the predicted output. Adjustments to the weights are made in response to the error observed at the output and the corresponding inputs to the neuron. This adjustment process is commonly referred to as backpropagation.

**Biases**  
Similar to weights, biases are adjustable parameters within a neural network. Each neuron receives an additional input known as the bias, which is consistently set to 1 and has its own associated weight. This design ensures that even when all other inputs are zero, the neuron can still produce an activation, thus introducing flexibility into the model.

**Activation Functions**  
Activation functions play a critical role in determining the output of a neural network. Each neuron in the network is linked to an activation function that decides whether the neuron should be activated based on the relevance of its input for the model's predictions. These functions also help normalize the output of each neuron, typically constraining it to a range between $0$ and $1$ or between $-1$ and $1$.

In summary, the essential components that form the foundation of neural networks in AI are nodes (or neurons), weights, biases, and activation functions. Together, these elements collaborate to process inputs and generate outputs, mirroring the functional processes of the human brain.
    