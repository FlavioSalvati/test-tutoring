---
title: "What is the difference between a range and a distribution?"
summary: "A range measures the difference between the highest and lowest values in a set, while a distribution describes how those values are spread out within that set."
author: "Dr. Laura Mitchell"
degree: "PhD in Molecular Biology, University of Oxford"
tutor_type: "A-Level Biology Tutor"
date: 2024-04-26
---

The range is defined as the difference between the highest and lowest values in a dataset, while a distribution describes how these values are spread out.

In statistics, the range serves as a measure of dispersion, indicating the extent of variation within a set of data. It is calculated by subtracting the lowest value from the highest value. For instance, if a set of test scores spans from $60$ to $90$, the range can be determined as follows:

$$
\text{Range} = \text{Highest Value} - \text{Lowest Value} = 90 - 60 = 30.
$$

Although the range is a straightforward measure of dispersion that is simple to compute, it can be significantly influenced by outliers.

Conversely, the term distribution refers to the way in which the values in a dataset are arranged. It conveys essential information regarding the shape, center, and spread of the data. Various types of distributions exist, including normal, skewed, and bimodal distributions. The form of the distribution can be impacted by several factors, such as sample size, measurement error, and the underlying processes that produce the data.

Grasping the distinction between range and distribution is crucial for effective data analysis. While the range offers a basic measure of dispersion, it does not provide insights into the shape of the data. In contrast, understanding the distribution provides a more comprehensive view of how the values are spread out, facilitating better inferences and conclusions regarding the dataset.
    